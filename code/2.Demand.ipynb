{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0304b825",
   "metadata": {},
   "source": [
    "# Delta Water Unavailability Methodology Module 2: Demand Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2eb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script calls input datasets of water rights and demands in the Sacramento-San Joaquin Delta Watershed\n",
    "# to prepare them for the water unavailability analysis.\n",
    "# Further documentation is available on the Delta Water Unavailability Methodology website:\n",
    "# https://www.waterboards.ca.gov/waterrights/water_issues/programs/drought/drought_tools_methods/delta_method.html\n",
    "\n",
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import calendar\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if output folders exist\n",
    "def check_folder(folder_path):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: Required folder '{folder_path}' does not exist. Please ensure 1)Supply has been run successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "# Check folders\n",
    "check_folder('./intermediate-outputs')\n",
    "check_folder('./output-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00e405",
   "metadata": {},
   "source": [
    "## A)  Water Rights Demands\n",
    "### i) User-Specified Analysis Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Start and End Date, and Exceedance variables from 1.Supply script\n",
    "try:\n",
    "    %store -r start_date\n",
    "    %store -r end_date\n",
    "    %store -r user_excd\n",
    "except NameError as e:\n",
    "    print(f\"Warning: {str(e)}. Please ensure 1)Supply has been run successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the year of reported demand data that will be used (currently only 2018 or 2019 available)\n",
    "while True:\n",
    "    demand_year = input(\"Enter a Demand Year (2018 or 2019): \\n\")\n",
    "    \n",
    "    if demand_year == \"2018\" or demand_year == \"2019\":\n",
    "        break  # Exit the loop if the user enters a valid value\n",
    "    \n",
    "    print(\"Invalid input. Please enter 2018 or 2019.\")\n",
    "\n",
    "print(\"You've selected Demand Year\", demand_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3017ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if Enhanced Reporting demands will be used in the analysis\n",
    "while True:\n",
    "    use_enhanced_reporting = input(\"Use Enhanced Reporting? Please enter 'yes' or 'no': \").strip().upper()\n",
    "    \n",
    "    if use_enhanced_reporting == 'YES' or use_enhanced_reporting == 'NO':\n",
    "        print(\"You've selected\", use_enhanced_reporting, \"on using Enhanced Reporting data\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid input. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_inputs = pd.DataFrame({\n",
    "    'Start Date': [start_date],\n",
    "    'End Date': [end_date],\n",
    "    '% Exceedance': [user_excd],\n",
    "    'Demand Year': [demand_year],\n",
    "    'Use Enhanced Reporting?': [use_enhanced_reporting]\n",
    "})\n",
    "#Export all user-specified Variables\n",
    "scenario_inputs.to_csv('./output-data/Scenario_Inputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d6fca",
   "metadata": {},
   "source": [
    "### ii) Import Water Rights dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c13b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset can contain different water rights, but expected headers should match sample input\n",
    "water_rights = pd.read_csv('../user-inputs/WaterRights.csv')\n",
    "water_rights.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ebb26",
   "metadata": {},
   "source": [
    "### iii) Import Demands dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand dataset must contain the same water rights as the Water Rights dataset, with expected headers\n",
    "demand = pd.read_csv('../user-inputs/Demands.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a3f8f",
   "metadata": {},
   "source": [
    "#### Filter 2018 and 2019 Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e81ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically selects and renames demand data columns based on the specified year\n",
    "def select_and_rename_demand_columns(demand_df, year):\n",
    "    dir_pattern = f'DIR_QAQC_{year}_{{month:02d}}_AF'\n",
    "    stor_pattern = f'STOR_QAQC_{year}_{{month:02d}}_AF'\n",
    "    \n",
    "    columns_selection = ['APPL_ID'] + [dir_pattern.format(month=month) for month in range(1, 13)] + [stor_pattern.format(month=month) for month in range(1, 13)]\n",
    "    demand_selected = demand_df[columns_selection].copy()\n",
    "\n",
    "    new_column_names = {**{dir_pattern.format(month=month): f'{calendar.month_abbr[month]} Direct (AF)' for month in range(1, 13)},\n",
    "                        **{stor_pattern.format(month=month): f'{calendar.month_abbr[month]} Storage (AF)' for month in range(1, 13)}}\n",
    "    \n",
    "    # Rename columns\n",
    "    demand_selected.rename(columns=new_column_names, inplace=True)\n",
    "    return demand_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ce902",
   "metadata": {},
   "source": [
    "### iv) Import Enhanced Reporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0eba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All months of Enhanced Reporting data that are available are imported.\n",
    "If a water right appears in a month’s Enhanced Reporting dataset and its NO_CHANGES value = FALSE,\n",
    "then the Enhanced Reporting value (direct or storage) is used for that month’s demand. \n",
    "'''\n",
    "# Grab demand data based on specified Demand Year \n",
    "period = select_and_rename_demand_columns(demand, demand_year)\n",
    "\n",
    "if use_enhanced_reporting == 'YES':\n",
    "    # Define available months of Enhanced Reporting data (November 2022-March 2023)\n",
    "    update_columns = ['Direct (AF)', 'Storage (AF)']\n",
    "    update_months = ['Nov', 'Dec', 'Jan', 'Feb', 'Mar']\n",
    "    all_update_cols = [f'{month} {col}' for month in update_months for col in update_columns]\n",
    "\n",
    "    # Identify the months within the date range that are available for enhanced reporting\n",
    "    analysis_months = [datetime.strftime(date, \"%b\") for date in pd.date_range(start_date, end_date, freq='M').unique()]\n",
    "    analysis_months = [month for month in analysis_months if month in update_months]\n",
    "\n",
    "    # Path to quality-controlled Enhanced Reporting data\n",
    "    path = '../user-inputs/Enhanced Reporting/*.csv'\n",
    "    \n",
    "    # Loop through each available Enhanced Reporting dataset\n",
    "    for file_path in glob.glob(path):\n",
    "        df = pd.read_csv(file_path, encoding='cp1252')\n",
    "        file_months = set(col.split(' ')[0] for col in df.columns if any(month in col for month in update_months))\n",
    "        \n",
    "        if not file_months.intersection(set(analysis_months)):\n",
    "            continue  # Skip this file if its month is not within the analysis_months\n",
    "\n",
    "        # Loop through the demand data to replace with Enhanced Reporting data where appropriate\n",
    "        for update_col in [col for col in all_update_cols if col.split(' ')[0] in analysis_months]:\n",
    "            month, data_type = update_col.split(' ', 1)\n",
    "            for index, row in df.iterrows():\n",
    "                appl_id = row['Application ID']\n",
    "                if not row.get('No Changes?', True): \n",
    "                    period.loc[period['APPL_ID'] == appl_id, update_col] = row[update_col]\n",
    "                else:\n",
    "                    demand_2018 = select_and_rename_demand_columns(demand, \"2018\")\n",
    "                    demand_2018 = demand_2018.loc[demand_2018['APPL_ID'] == appl_id]\n",
    "                    if not demand_2018.empty:\n",
    "                        period.loc[period['APPL_ID'] == appl_id, update_col] = demand_2018[update_col].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb7f86",
   "metadata": {},
   "source": [
    "### v) Calculate Period Direct and Period Storage Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd74f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for direct and storage demands can be identified by their naming convention.\n",
    "direct_columns = [col for col in period.columns if 'Direct' in col]\n",
    "storage_columns = [col for col in period.columns if 'Storage' in col]\n",
    "\n",
    "# Selecting appropriate columns for calculation based on their type\n",
    "period_direct = period[['APPL_ID'] + direct_columns]\n",
    "period_storage = period[['APPL_ID'] + storage_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename columns to match previous conventions\n",
    "def rename_to_month_abbr(df):\n",
    "    new_column_names = {col: col.split(' ')[0] for col in df.columns if col != 'APPL_ID'}\n",
    "    return df.rename(columns=new_column_names)\n",
    "\n",
    "# Apply the function to both DataFrames\n",
    "period_direct = rename_to_month_abbr(period_direct)\n",
    "period_storage = rename_to_month_abbr(period_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4da64",
   "metadata": {},
   "source": [
    "#### Prorate both Period Direct and Period Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that intakes the specified Start and End Dates to calculate a multiplier\n",
    "# The multiplier is used to prorate multiple months of data down to a single value for the specified dates\n",
    "def calculate_prorated_constant(start_date, end_date, constants):\n",
    "    # Get the number of days in a month\n",
    "    def days_in_month(year, month):\n",
    "        return calendar.monthrange(year, month)[1]\n",
    "\n",
    "    # Initialize the prorated_value\n",
    "    prorated_value = 0.0\n",
    "\n",
    "    current_date = start_date\n",
    "    # Add prorated value for the start month\n",
    "    prorated_value += (days_in_month(start_date.year, start_date.month) - start_date.day + 1) / days_in_month(start_date.year, start_date.month) * constants[start_date.month - 1]\n",
    "\n",
    "    # Increment month by month until the end date is reached\n",
    "    while True:\n",
    "        next_month_first_day = (current_date.replace(day=28) + timedelta(days=4)).replace(day=1) # To safely get the first day of next month\n",
    "        if next_month_first_day >= end_date:\n",
    "            break\n",
    "        \n",
    "        # Add prorated value for full month or fraction for partial month\n",
    "        if next_month_first_day.month == end_date.month:\n",
    "            prorated_value += end_date.day / days_in_month(end_date.year, end_date.month) * constants[end_date.month - 1]\n",
    "        else:\n",
    "            prorated_value += constants[next_month_first_day.month - 1]\n",
    "        current_date = next_month_first_day\n",
    "\n",
    "    return prorated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014dfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the proration function to transform monthly Direct values into a single Period Direct value based on specified Start and End Dates\n",
    "period_direct_prorated = []\n",
    "period_direct_dict = period_direct.set_index('APPL_ID').T.to_dict('list')\n",
    "\n",
    "for identifier, constants in period_direct_dict.items():\n",
    "    constant = calculate_prorated_constant(start_date, end_date, constants)\n",
    "    period_direct_prorated.append({'APPL_ID': identifier, 'DIRECT_PERIOD': constant})\n",
    "\n",
    "period_direct_prorated = pd.DataFrame(period_direct_prorated)\n",
    "\n",
    "#display(period_direct_prorated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f164e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the proration function to transform monthly Storage values into a single Period Storage value based on specified Start and End Dates\n",
    "period_storage_prorated = []\n",
    "period_storage_dict = period_storage.set_index('APPL_ID').T.to_dict('list')\n",
    "\n",
    "for identifier, constants in period_storage_dict.items():\n",
    "    constant = calculate_prorated_constant(start_date, end_date, constants)\n",
    "    period_storage_prorated.append({'APPL_ID': identifier, 'STORAGE_PERIOD': constant})\n",
    "\n",
    "period_storage_prorated = pd.DataFrame(period_storage_prorated)\n",
    "\n",
    "#display(period_storage_prorated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d79fd0",
   "metadata": {},
   "source": [
    "## B)  POD Demands\n",
    "### i) Import POD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD dataset must contain the same water rights as the Water Rights dataset, with expected headers\n",
    "pods = pd.read_csv('../user-inputs/PODs.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7631413",
   "metadata": {},
   "source": [
    "### ii) Import Consumptive Use Factors (CUF) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a678aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consmuptive Use Factors are used to reduce Direct demands to acocunt for return flows by month and subwatershed\n",
    "use_factors = pd.read_csv('../user-inputs/ConsumptiveUseFactors.csv')\n",
    "\n",
    "# Rename columns to match previous conventions\n",
    "use_factors = use_factors.rename(columns={\n",
    "    **{f'{i:02d}_RATIO': calendar.month_abbr[i] for i in range(1, 13)}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122bf0d",
   "metadata": {},
   "source": [
    "### iii) Calculate a Period CUF based on user-specified Start/End Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the proration function to transform monthly CUF valuess into single Period CUF values based on specified Start and End Dates\n",
    "prorated_cuf = []\n",
    "\n",
    "for subwatershed in use_factors['SUBWATERSHED']:\n",
    "    constant = calculate_prorated_constant(start_date, end_date, use_factors.loc[use_factors['SUBWATERSHED'] == subwatershed].drop('SUBWATERSHED', axis=1).squeeze())\n",
    "    prorated_cuf.append({'Period_CUF_Constant': constant, 'SUBWATERSHED': subwatershed})\n",
    "\n",
    "period_cuf = pd.DataFrame(prorated_cuf)\n",
    "\n",
    "#display(period_cuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80054ab",
   "metadata": {},
   "source": [
    "### iv) POD Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d182e6",
   "metadata": {},
   "source": [
    "###### 1) POD Period Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd318031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Direct demands for each POD by merging from the water right period demand dataset based on its Application ID\n",
    "merged_df = pd.merge(pods, period_direct_prorated[['APPL_ID', 'DIRECT_PERIOD']], on='APPL_ID')\n",
    "\n",
    "# Multiply POD Direct demands by CUF values based on 'SUBWATERSHED'\n",
    "final_df = pd.merge(merged_df, period_cuf, how='left', on='SUBWATERSHED')\n",
    "\n",
    "# Weight the water right's CUF-reduced direct demand to the individual POD\n",
    "final_df['DIRECT_PERIOD_POD'] = final_df['WEIGHT_DIR'] * final_df['DIRECT_PERIOD'] * final_df['Period_CUF_Constant']\n",
    "\n",
    "# Create the final POD Direct demand dataframe with the desired columns\n",
    "direct_period_pod = final_df[['APPL_ID', 'POD_ID', 'WATERSHED', 'SUBWATERSHED', 'LEGAL_DELTA', 'WEIGHT_DIR', 'DIRECT_PERIOD', 'Period_CUF_Constant', 'DIRECT_PERIOD_POD']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0132a",
   "metadata": {},
   "source": [
    "###### 2) POD Period Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Storage demands for each POD by merging from the water right period demand dataset based on its Application ID\n",
    "merged_df2 = pd.merge(pods, period_storage_prorated[['APPL_ID', 'STORAGE_PERIOD']], on='APPL_ID')\n",
    "\n",
    "# Storage demands are not multiplied by CUF factors since diversions to storage do not produce return flows\n",
    "\n",
    "# Weight the water right's storage demand to the individual POD\n",
    "merged_df2['STORAGE_PERIOD_POD'] = merged_df2['WEIGHT_STOR'] * merged_df2['STORAGE_PERIOD']\n",
    "\n",
    "# Create the final POD Storage demand dataframe with the desired columns\n",
    "storage_period_pod = merged_df2[['APPL_ID', 'POD_ID', 'WATERSHED', 'SUBWATERSHED', 'STORAGE_PERIOD', 'WEIGHT_STOR', 'STORAGE_PERIOD_POD']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3fe5c",
   "metadata": {},
   "source": [
    "###### 3) POD Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9567cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the POD Direct and Storage dataframes to create a final POD Period Demand dataframe\n",
    "demand_period_pod = pd.DataFrame()\n",
    "demand_period_pod = pods[['APPL_ID','POD_ID','WATERSHED', 'SUBWATERSHED', 'LEGAL_DELTA']].copy()\n",
    "demand_period_pod['DIRECT_PERIOD_POD'] = direct_period_pod['DIRECT_PERIOD_POD']\n",
    "demand_period_pod['STORAGE_PERIOD_POD'] = storage_period_pod['STORAGE_PERIOD_POD']\n",
    "demand_period_pod['DEMAND_PERIOD_POD'] = direct_period_pod['DIRECT_PERIOD_POD'] + storage_period_pod['STORAGE_PERIOD_POD']\n",
    "\n",
    "#display(demand_period_pod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'WATER_RIGHT_TYPE_CUSTOM' column from water_rights dataframe\n",
    "demand_period_pod = demand_period_pod.merge(water_rights[['APPL_ID', 'WATER_RIGHT_TYPE_CUSTOM']], on='APPL_ID', how='left')\n",
    "\n",
    "# Export total POD Period Demands, to be called in 3.Analysis script\n",
    "demand_period_pod.to_csv('./intermediate-outputs/Demands_Period_POD.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2824e",
   "metadata": {},
   "source": [
    "## C) Analysis Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea464d",
   "metadata": {},
   "source": [
    "### Create the Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Analysis dataset by flattening the PODs dataset to only rows that have a unique combination of Appl_ID, Watershed, Subwatershed, and Legal_Delta values\n",
    "selected_columns = ['APPL_ID', 'POD_ID', 'WATERSHED', 'SUBWATERSHED', 'LEGAL_DELTA', 'WEIGHT_DIR', 'WEIGHT_STOR']\n",
    "flat_df = pods[selected_columns].copy()\n",
    "\n",
    "# Compute the sum of direct and storage weights for each Watershed-Subwatershed-Legal_Delta combination\n",
    "sum_weights = flat_df.groupby(['APPL_ID', 'WATERSHED', 'SUBWATERSHED', 'LEGAL_DELTA'], as_index=False).agg({\n",
    "    'WEIGHT_DIR': 'sum',\n",
    "    'WEIGHT_STOR': 'sum'\n",
    "})\n",
    "\n",
    "'''\n",
    "If the sum of Direct and Storage Weights is zero, remove the record from the Analysis dataset. \n",
    "This is necessary because areas with only points of rediversion or inactive \n",
    "PODs are not considered when evaluating water unavailability.\n",
    "'''\n",
    "\n",
    "sum_weights = sum_weights.loc[(sum_weights['WEIGHT_DIR'] != 0) | (sum_weights['WEIGHT_STOR'] !=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8517f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Priority Date, Primary Owner, Water Right Type, values from WaterRights dataset\n",
    "analysis_df = pd.merge(water_rights, sum_weights, on='APPL_ID', how='left')\n",
    "\n",
    "# Remove any records with a Priority Date of Riparian from the Analysis dataset, as these will be evaluated collectively\n",
    "analysis_df = analysis_df.loc[analysis_df['PRIORITY_DATE_CUSTOM'] != 'Riparian']\n",
    "analysis_df = analysis_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort records based on Subwatershed and Legal Delta as listed below\n",
    "sort_order = {\n",
    "    ('Sacramento Bend', False): 1,\n",
    "    ('Upper Sacramento Valley', False): 2,\n",
    "    ('Stony', False): 3,\n",
    "    ('Cache', False): 4,\n",
    "    ('Upper Feather', False): 5,\n",
    "    ('Yuba', False): 6,\n",
    "    ('Bear', False): 7,\n",
    "    ('Upper American', False): 8,\n",
    "    ('Sacramento Valley Floor', False): 9,\n",
    "    ('Upper San Joaquin', False): 10,\n",
    "    ('Fresno', False): 11,\n",
    "    ('Chowchilla', False): 12,\n",
    "    ('Merced', False): 13,\n",
    "    ('Tuolumne', False): 14,\n",
    "    ('San Joaquin Valley Floor', False): 15,\n",
    "    ('Putah', False): 16,\n",
    "    ('Stanislaus', False): 17,\n",
    "    ('Calaveras', False): 18,\n",
    "    ('Mokelumne', False): 19,    \n",
    "    ('Cosumnes', False): 20,\n",
    "    ('Putah', True): 21,\n",
    "    ('Stanislaus', True): 22,\n",
    "    ('Calaveras', True): 23,\n",
    "    ('Mokelumne', True): 24,    \n",
    "    ('Cosumnes', True): 25,\n",
    "    ('Sacramento Valley Floor', True): 26,\n",
    "    ('San Joaquin Valley Floor', True): 27,\n",
    "}\n",
    "\n",
    "# Create a custom sorting key for subwatershed and Legal Delta\n",
    "def custom_sort_key2(row):\n",
    "    return sort_order.get((row['SUBWATERSHED'], row['LEGAL_DELTA']), 0)\n",
    "\n",
    "# Apply the custom sorting key to create a sort column\n",
    "analysis_df['SORT_KEY'] = analysis_df.apply(custom_sort_key2, axis=1)\n",
    "\n",
    "# Sort the DataFrame by the custom sort key and then by APPL_ID\n",
    "analysis_df = analysis_df.sort_values(by=['SORT_KEY', 'APPL_ID'])\n",
    "\n",
    "# Drop the temporary column used for sorting\n",
    "analysis_df.drop(columns=['SORT_KEY'], inplace=True)\n",
    "\n",
    "# Reset index if needed\n",
    "analysis_df = analysis_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort records based on Application ID then Priority Date\n",
    "# If 2 records have the same priority date, the first numbered application is assumed to be senior\n",
    "analysis_df = analysis_df.sort_values(by=['PRIORITY_DATE_CUSTOM', 'APPL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13727955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom sorting key to ensure Project-priority records go second-to-last and Pending-priority records go last\n",
    "def custom_sort_key(date):\n",
    "    if date == 'Project':\n",
    "        return 2\n",
    "    elif date == 'Pending':\n",
    "        return 3\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply the key to Priority Date\n",
    "analysis_df['SORT_KEY'] = analysis_df['PRIORITY_DATE_CUSTOM'].apply(custom_sort_key)\n",
    "\n",
    "# Sort the dataframe by Application ID then Sort Key\n",
    "sorted_analysis_df = analysis_df.sort_values(by=['SORT_KEY', 'PRIORITY_DATE_CUSTOM', 'APPL_ID'])\n",
    "\n",
    "# Drop the temporary column used for sorting\n",
    "sorted_analysis_df.drop(columns=['SORT_KEY'], inplace=True)\n",
    "\n",
    "sorted_analysis_df = sorted_analysis_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map a Headwater (T/F) value based on the Subwatershed\n",
    "def assign_headwater(subwatershed):\n",
    "    if subwatershed in ['Upper Sacramento Valley', 'Sacramento Valley Floor', 'San Joaquin Valley Floor']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "# Create a new Headwater column\n",
    "sorted_analysis_df['HEADWATER'] = sorted_analysis_df['SUBWATERSHED'].apply(assign_headwater)\n",
    "\n",
    "# Re-arrange columns to match previous conventions\n",
    "analysis_dataset = sorted_analysis_df[['WATERSHED', 'SUBWATERSHED', 'APPL_ID', 'PRIMARY_OWNER_NAME', 'WATER_RIGHT_TYPE_CUSTOM', 'PRIORITY_DATE_CUSTOM', 'LEGAL_DELTA', 'HEADWATER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1256d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 81 rows of the Analysis dataset represent aggregations of Riparian-priority claims by sub-type and subwatershed\n",
    "# Import pre-defined aggregation list\n",
    "prior = pd.read_csv('../user-inputs/AnalysisPreparationRiparian.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93609f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Watershed values to Riparian-priority records\n",
    "# Function to map Watershed values based on Subwatershed\n",
    "def assign_watershed(subwatershed):\n",
    "    if subwatershed in ['Sacramento Bend', 'Stony', 'Cache', 'Upper Feather', 'Yuba', 'Bear',\n",
    "                        'Upper American', 'Putah', 'Upper Sacramento Valley', 'Sacramento Valley Floor']:\n",
    "        return 'Sacramento'\n",
    "    else:\n",
    "        return 'San Joaquin'\n",
    "    \n",
    "# Create a new Watershed column\n",
    "prior['WATERSHED'] = prior['SUBWATERSHED'].apply(assign_watershed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the list of aggregated Riparian-priority claims at the beginning of the Analysis dataset\n",
    "analysis_dataset = pd.concat([prior, analysis_dataset], keys=['df1', 'df2'], ignore_index=True)\n",
    "analysis_dataset = analysis_dataset[['WATERSHED', 'SUBWATERSHED', 'APPL_ID', 'PRIMARY_OWNER_NAME', 'WATER_RIGHT_TYPE_CUSTOM', 'PRIORITY_DATE_CUSTOM', 'LEGAL_DELTA', 'HEADWATER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db766539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export intermediate results for use in 3.Analysis script\n",
    "analysis_dataset.to_csv('./intermediate-outputs/Analysis_Dataset_Pre.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
